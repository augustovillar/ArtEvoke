# 📊 LMM Evaluation via Cosine Similarity

This script evaluates image descriptions generated by various **Large Multimodal Models (LMMs)** by comparing them to ground-truth textual descriptions using cosine similarity over sentence embeddings.

---

## 📁 Folder Structure
```
project/
├── eval_env/ # Virtual environment
│   ├── ...
├── llama/
│ └── llama_output.csv # Generated by running LLaMA script
├── llava/
│ └── llava_output.csv # Generated by running LLaVA script
├── qwen/
│ └── qwen_output.csv # Generated by running Qwen script
├── evaluation/
│ └── evaluating.py # This evaluation script
│ └── results_cos_sim.csv # (Generated) Cosine similarity results
│ └── results_stats.csv # (Generated) Statistical summary
```
## 🛠️ Installation

It's recommended to use a virtual environment:

```bash
# Create virtual environment
python -m venv eval_env
source eval_env/bin/activate  # On Windows use: eval_env\Scripts\activate

pip install -r requirements.txt
```

## 🧠 **Models Required for Evaluation**
Before running this evaluation script, make sure the outputs from the following models are available:

- `llama_output.csv` from **LLaMA**
- `llava_output.csv` from **LLaVA**
- `qwen_output.csv` from **Qwen 2.5 VL**

> 📌 Each of these files must be generated beforehand by executing the appropriate scripts in the `llama/`, `llava/`, and `qwen/` directories.

---

## 🚀 **How to Use This Script**
Navigate to the `LMMs/` folder and follow the steps below.

### 1. **Create Cosine Similarity Scores**
```bash
python evaluating.py create
```
This step will:
- Combine outputs from LLaMA, LLaVA, and Qwen
- Generate sentence embeddings using four pre-defined models
- Calculate cosine similarity between original and generated descriptions
- Save the result in `results_cos_sim.csv`

### 2. **Evaluate All Embedding Models**
```bash
python evaluating.py evaluate
```
This command generates descriptive statistics for each model:
- **Mean**, **median**, **standard deviation**, **minimum**, and **maximum**
- For each generated description model
- Saves the output to `results_stats.csv`

### 3. **Check Runtime and GPU Memory Usage**
```bash
python evaluating.py runtime_info
```
This will report:
- Average **processing time**
- Average **GPU memory allocated and reserved** per model

### 4. **Run Statistical Comparison Tests**
```bash
python evaluating.py t-test
```
This command performs:
- **Paired t-tests** and **Wilcoxon signed-rank tests**
- **Shapiro-Wilk tests** for normality
- To compare the quality of generated descriptions across models


### 5. **(Optional) Evaluate a Specific Embedding Model**
```bash
python evaluating.py evaluate gte-large
```
Replace `gte-large` with one of the following model identifiers:
- `bge-large-en-v1.5`
- `MiniLM-L6-v2`
- `gte-large`
- `e5-large-v2`

---

## 📂 **Output Files Generated**
- `results_cos_sim.csv`: Cosine similarity scores between reference and generated descriptions
- `results_stats.csv`: Summary statistics per embedding model
- Statistical test results are printed to the terminal

---

## 📬 **Important Notes**
- Ensure all `.csv` output files (`llava_output.csv`, `llama_output.csv`, `qwen_output.csv`) are generated **before** running the evaluation.
- This evaluation script benchmarks how well multimodal models describe visual content by comparing their output to human-written descriptions.
