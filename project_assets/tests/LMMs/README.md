# ðŸ“Š LMM Evaluation via Cosine Similarity

This script evaluates image descriptions generated by various **Large Multimodal Models (LMMs)** by comparing them to ground-truth textual descriptions using cosine similarity over sentence embeddings.

---

## ðŸ“ Folder Structure
```
project/
â”œâ”€â”€ eval_env/ # Virtual environment
â”‚   â”œâ”€â”€ ...
â”œâ”€â”€ llama/
â”‚ â””â”€â”€ llama_output.csv # Generated by running LLaMA script
â”œâ”€â”€ llava/
â”‚ â””â”€â”€ llava_output.csv # Generated by running LLaVA script
â”œâ”€â”€ qwen/
â”‚ â””â”€â”€ qwen_output.csv # Generated by running Qwen script
â”œâ”€â”€ evaluation/
â”‚ â””â”€â”€ evaluating.py # This evaluation script
â”‚ â””â”€â”€ results_cos_sim.csv # (Generated) Cosine similarity results
â”‚ â””â”€â”€ results_stats.csv # (Generated) Statistical summary
```
## ðŸ› ï¸ Installation

It's recommended to use a virtual environment:

```bash
# Create virtual environment
python -m venv eval_env
source eval_env/bin/activate  # On Windows use: eval_env\Scripts\activate

pip install -r requirements.txt
```

## ðŸ§  **Models Required for Evaluation**
Before running this evaluation script, make sure the outputs from the following models are available:

- `llama_output.csv` from **LLaMA**
- `llava_output.csv` from **LLaVA**
- `qwen_output.csv` from **Qwen 2.5 VL**

> ðŸ“Œ Each of these files must be generated beforehand by executing the appropriate scripts in the `llama/`, `llava/`, and `qwen/` directories.

---

## ðŸš€ **How to Use This Script**
Navigate to the `LMMs/` folder and follow the steps below.

### 1. **Create Cosine Similarity Scores**
```bash
python evaluating.py create
```
This step will:
- Combine outputs from LLaMA, LLaVA, and Qwen
- Generate sentence embeddings using four pre-defined models
- Calculate cosine similarity between original and generated descriptions
- Save the result in `results_cos_sim.csv`

### 2. **Evaluate All Embedding Models**
```bash
python evaluating.py evaluate
```
This command generates descriptive statistics for each model:
- **Mean**, **median**, **standard deviation**, **minimum**, and **maximum**
- For each generated description model
- Saves the output to `results_stats.csv`

### 3. **Check Runtime and GPU Memory Usage**
```bash
python evaluating.py runtime_info
```
This will report:
- Average **processing time**
- Average **GPU memory allocated and reserved** per model

### 4. **Run Statistical Comparison Tests**
```bash
python evaluating.py t-test
```
This command performs:
- **Paired t-tests** and **Wilcoxon signed-rank tests**
- **Shapiro-Wilk tests** for normality
- To compare the quality of generated descriptions across models


### 5. **(Optional) Evaluate a Specific Embedding Model**
```bash
python evaluating.py evaluate gte-large
```
Replace `gte-large` with one of the following model identifiers:
- `bge-large-en-v1.5`
- `MiniLM-L6-v2`
- `gte-large`
- `e5-large-v2`

---

## ðŸ“‚ **Output Files Generated**
- `results_cos_sim.csv`: Cosine similarity scores between reference and generated descriptions
- `results_stats.csv`: Summary statistics per embedding model
- Statistical test results are printed to the terminal

---

## ðŸ“¬ **Important Notes**
- Ensure all `.csv` output files (`llava_output.csv`, `llama_output.csv`, `qwen_output.csv`) are generated **before** running the evaluation.
- This evaluation script benchmarks how well multimodal models describe visual content by comparing their output to human-written descriptions.
